= Flux and GitOps: What, Why and How
:page-excerpt: Flux and GitOps simplify Kubernetes management by defining your clusterâ€™s desired state in Git, ensuring consistency, security, and easy rollbacks.
:page-tags: [flux, gitops]
:page-published: true

In the link:/2024/11/21/bootstrap-your-home-server-with-kubernetes-and-flux.html[previous post], we set up a Kubernetes cluster on our home server using Ansible. We installed Flux to manage app deployments and cluster configurations. But we didn't discuss __why__ we're using Flux, what GitOps is, and how it  fits together.

== The Problem(s)

If you've worked in a DevOps environment (where the developers who write the code are also responsible for deploying it, supporting it, etc.), I'd bet money that your deployment process looks something like this:

[plantuml, format=svg]
----
autoactivate on
Actor Developer
Participant "Git Repository" as Git
Participant "CI/CD Pipeline" as Pipeline
Participant "Container Registry" as Registry
Participant "Kubernetes Cluster" as Cluster

Developer -> Git: Push code
return Pushed

Git -> Pipeline: Trigger build
Pipeline -> Registry: Test, build and push image
return Image pushed
return Build successful

...Optional approval process...

Developer -> Pipeline: Trigger release
Pipeline -> Cluster: Update manifest/chart and apply

Cluster -> Cluster: Reconcile changes
return Changed reconciled
return Apply complete

return Release successful
----

Beautiful, almost nostalgic. But there are a few problems with it:

. What if the cluster wasn't in the state you expected it to be in?
.. It's probably happened to you. You're doing a routine deployment, but the pipeline fails because the pods didn't reach a ready state. You check the deployment manifest, turns out your favourite colleague was testing stuff and added `replicas: 0` but forgot to take it back out (again). Or worse, the namespace you're deploying to no longer exists! ðŸ”¥
+
. Now that the pipeline has failed and the cluster is in a failed state, what do you do?
.. You had a fancy pipeline to abstract away and automate the whole deployment process, but now that you need to dive in you're stuck. Are you going to run `kubectl` commands against the cluster to see what's going on? Or run `kubectl delete deployment/<app>` to start fresh, and then re-run the pipeline? Hopefully you're not in an bridge call with 100 people watching your screen as you type `kubectl --help`. How __exactly__ do you rollback, what permissions do you need to do so, and then how do you __guarantee__ that the cluster is now in the state you expect it to be?
+
. You've heard of SecOps and you wear the T-shirt with pride. You ignore your therapist when she says not to apply the Zero Trust Model to everyday life. I think part of you knows how insecure this process it but don't want to admit it.
.. Not only does the CI/CD pipeline need write access to your cluster, but probably so does every developer on your team, because __when__ the pipeline fails, the cluster will require manual intervention to troubleshoot and fix. This means any successful phishing attack, or even a lost/stolen laptop, could spell game over.
+
. It's Friday evening and you've been called into an urgent meeting. "What is our DR?!" your manager shouts. Confused, you wonder if he's been drinking. "We probably see different doctors, boss".
.. Your cluster has disappeared. Most likely it was on Azure. You raise a platinum support ticket to get your money's worth but you know full well you're on your own. Now you're tasked with a rebuild. You'll need to duplicate/update all the CI/CD pipelines and monitoring/alerting rules. Create new service connections. Copy over (hopefully) existing manifests and configurations. Re-deploy all your applications (who knows what versions were actually live, it's definitely not in the manifest. You find the following pipeline code and your impostor syndrome climaxes: `sed -ie "s/IM_THE_VERSION_REPLACE_ME_LOL/$actualVersionLol/g" deployment.yml
kubectl apply -f deployment.yml`) How long will this all take? And let's not pretend you won't forget something.

So it seems the defacto solution for deployments is to somehow get some state into Kubernetes, and then make continuous alterations to that state, either manually or through pipelines. This makes it very hard to know what is the current state of the cluster, how to get back to a given state, and who has access to the cluster.

== The Solution

Imagine if you could define the desired state of your cluster in a Git repository, and that there was a tool that would monitor your cluster, and make sure it matched the state in Git. It would mean you could make changes to your cluster with a `git commit`. Changes would be auditable, and reversible. A rollback would be a `git revert`. Who has permission to make changes to the cluster is as simple as looking at the Git repository settings. This is what Flux offers us, and the process of driving change through Git is called GitOps.

This is what the same process above would look like using Flux and GitOps:

[plantuml, format=svg]
----
autoactivate on
Actor Developer
Participant "Git Repository" as Git
Participant "CI/CD Pipeline" as Pipeline
Participant "Container Registry" as Registry
Participant "Cluster State\nGit Repository" as CGit
Participant "Kubernetes Cluster" as Cluster

Developer -> Git: Push code
return Pushed

Git -> Pipeline: Trigger build
Pipeline -> Registry: Test, build and push image
return Image pushed
return Build successful

...Optional approval process...

Developer -> CGit: Push code
note left of CGit: This step is actually optional
return Pushed

activate Cluster
Cluster -> CGit: Poll for changes
return Difference detected in\ncurrent state and desired state

Cluster -> Cluster: Reconcile changes
return Changed reconciled
----

The diagram above says that the step where the developer pushes code for the second time is optional. Indeed, depending on how you set up Flux, a deployment could be as easy as pushing a new image. For example, let's say the following Flux resources have been created:

[source,yaml]
----
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: prometheus-community-charts
  namespace: observability
spec:
  interval: 24h <1>
  url: https://prometheus-community.github.io/helm-charts

---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: observability
spec:
  interval: 30m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: "61.x" <2>
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: observability
      interval: 12h
----
<1> Flux will check this Helm repository every 24 hours for new versions.
<2> If Flux detects a new version, e.g. `61.1`, it will automatically update the Helm release to use that version. This is so powerful. You simply define __what__ you want in your cluster, and Flux will make it so. By the end of this series we'll have several apps deployed, all of which will be being kept automatically up to date, with absolutely no effort on our side.

Let's see how this new process has solved the problems above:

. What state is the cluster in?
.. Look at the latest commit in the Git repository.
. How to get back to a given state?
.. `git revert`.
. Security?
.. We've closed the cluster off from the outside world. Flux runs __within__ the cluster, with read-only egress to the Git repository.
. Reproducibility?
.. The entire cluster state is in code in Git. You can install Flux on any other cluster and have it bootstrap from the Git repository to get back to the exact same state.

There are other benefits as well, like developers not needing to learn new tools or Kubernetes intricacies. Instead, they use an interface they're already familiar with: Git.

== Next Steps

In the next post we'll deploy our observability stack to our cluster using Flux: Prometheus, Grafana and Loki. Batteries included. We'll also be setting up our own alerting for when something goes wrong. https://x.com/cristianrgreco[Follow me on X to get updated!]
